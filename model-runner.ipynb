{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a75f14-a139-463a-99b6-4841a2022148",
   "metadata": {},
   "source": [
    "### Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37b0d4b-48a1-4fc6-a0c4-c6130f7c7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('secret_tokens.json') as f:\n",
    "    tokens_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0504e77d-67be-4b03-a054-83b2d1dc2518",
   "metadata": {
    "is_executing": true
   },
   "source": [
    "### CodeLLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30ff65f1-59dd-4c5e-8598-08b3b5d5aace",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(\u001b[43mwrapper\u001b[49m)\n\u001b[1;32m      3\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(clw)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wrapper' is not defined"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(clw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f241a8baeea1e8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b0384e899449adab4f695636e7979f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading multi-humaneval | language = kotlin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "1it [00:12, 13.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "2it [00:25, 12.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "3it [00:38, 12.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "4it [00:50, 12.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "5it [01:03, 12.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "6it [01:16, 12.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "7it [01:28, 12.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "8it [01:42, 12.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "9it [01:54, 12.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "10it [02:07, 12.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "11it [02:19, 12.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "12it [02:33, 12.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "13it [02:46, 12.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "14it [03:00, 13.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "15it [03:14, 13.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "16it [03:28, 13.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "17it [03:42, 13.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "18it [03:52, 12.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "19it [04:06, 12.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "20it [04:18, 12.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "21it [04:29, 12.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "22it [04:41, 12.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "23it [04:55, 12.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "24it [05:09, 13.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "25it [05:24, 13.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "26it [05:36, 13.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "27it [05:50, 13.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "28it [06:04, 13.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "29it [06:19, 13.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "30it [06:32, 13.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "31it [06:44, 13.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "32it [06:56, 12.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "33it [07:07, 12.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "34it [07:21, 12.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "35it [07:35, 12.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "36it [07:48, 13.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "37it [08:00, 12.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "38it [08:12, 12.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "39it [08:22, 11.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "40it [08:33, 11.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "41it [08:45, 11.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "42it [08:55, 11.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "43it [09:07, 11.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "44it [09:22, 12.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "45it [09:32, 11.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "46it [09:46, 12.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "47it [09:59, 12.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "48it [10:11, 12.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "49it [10:23, 12.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "50it [10:36, 12.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "51it [10:50, 13.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "52it [11:01, 12.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "53it [11:15, 12.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "54it [11:28, 12.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "55it [11:41, 12.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "56it [11:53, 12.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "57it [12:06, 12.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "58it [12:19, 12.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "59it [12:32, 12.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "60it [12:44, 12.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "61it [12:54, 12.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "62it [13:06, 11.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "63it [13:19, 12.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "64it [13:31, 12.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "65it [13:39, 10.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "66it [13:42,  8.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "67it [13:53,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "68it [14:04,  9.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "69it [14:16, 10.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "70it [14:23,  9.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "71it [14:34,  9.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "72it [14:42,  9.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "73it [14:56, 10.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "74it [15:07, 10.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "75it [15:19, 11.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "76it [15:23,  9.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "77it [15:34,  9.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "78it [15:46, 10.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "79it [15:51,  8.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "80it [16:05, 10.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "81it [16:19, 11.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "82it [16:30, 11.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "83it [16:44, 12.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "84it [16:55, 11.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "85it [17:02, 10.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "86it [17:10,  9.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "87it [17:22, 10.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "88it [17:34, 10.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "89it [17:46, 11.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "90it [17:57, 11.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "91it [18:09, 11.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "92it [18:14,  9.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "93it [18:23,  9.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "94it [18:33,  9.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "95it [18:45, 10.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "96it [18:59, 11.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "97it [19:09, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "98it [19:20, 11.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "99it [19:32, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100it [19:45, 11.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "101it [19:55, 11.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "102it [20:07, 11.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "103it [20:12,  9.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "104it [20:24, 10.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "105it [20:32,  9.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "106it [20:43, 10.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "107it [20:48,  8.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "108it [20:57,  8.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "109it [21:07,  9.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "110it [21:18,  9.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "111it [21:27,  9.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "112it [21:40, 10.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "113it [21:44,  8.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "114it [21:54,  8.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "115it [22:03,  9.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "116it [22:13,  9.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "117it [22:24,  9.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "118it [22:31,  9.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "119it [22:44, 10.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "120it [22:55, 10.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "121it [23:03,  9.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "122it [23:09,  8.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "123it [23:20,  9.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "124it [23:27,  8.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "125it [23:34,  8.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "126it [23:46,  9.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "127it [23:47,  6.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "128it [23:54,  7.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "129it [24:08,  9.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "130it [24:19,  9.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "131it [24:30,  9.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "132it [24:40, 10.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "133it [24:53, 10.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "134it [25:04, 10.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "135it [25:15, 10.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "136it [25:28, 11.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "137it [25:41, 11.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "138it [25:53, 11.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "139it [26:01, 10.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "140it [26:11, 10.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "141it [26:22, 10.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "142it [26:32, 10.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "143it [26:44, 10.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "144it [26:56, 11.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "145it [27:06, 10.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "146it [27:13,  9.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "147it [27:23,  9.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "148it [27:35, 10.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "149it [27:46, 10.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "150it [27:54,  9.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "151it [28:00,  8.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "152it [28:11,  9.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "153it [28:25, 10.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "154it [28:37, 11.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "155it [28:49, 11.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "156it [29:01, 11.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "157it [29:05,  9.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "158it [29:13,  8.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "159it [29:25,  9.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "160it [29:38, 10.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "161it [29:50, 11.12s/it]\n"
     ]
    }
   ],
   "source": [
    "import codellama_wrapper as clw\n",
    "codellama_trial = clw.CodellamaEval(model_name = \"codellama/CodeLlama-7b-hf\", gpu_id=1)\n",
    "codellama_trial.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f00e98d8-4eda-4303-8495-878db904c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = codellama_trial.model_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb3a09a0-4c8f-4336-8c62-da739f54e168",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcodellama_trial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eval/mxeval/codellama_wrapper.py:66\u001b[0m, in \u001b[0;36mCodellamaEval.evaluate\u001b[0;34m(problem_name, model_outputs_jsonl, top_k, n_workers, timeout)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(problem_name: \u001b[38;5;28mstr\u001b[39m  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumaneval\u001b[39m\u001b[38;5;124m'\u001b[39m, model_outputs_jsonl: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputmulti-humaneval.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15.0\u001b[39m):\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_outputs_jsonl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eval/mxeval/wrapper.py:61\u001b[0m, in \u001b[0;36mKotlinLLMEval.evaluate\u001b[0;34m(self, problem_name, model_outputs_jsonl, top_k, n_workers, timeout)\u001b[0m\n\u001b[1;32m     59\u001b[0m     reference_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/mbxp/mbkp_release_v1.2.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m     62\u001b[0m mxeval\u001b[38;5;241m.\u001b[39mevaluation\u001b[38;5;241m.\u001b[39mevaluate_functional_correctness(\n\u001b[1;32m     63\u001b[0m     sample_file\u001b[38;5;241m=\u001b[39mmodel_outputs_jsonl, k\u001b[38;5;241m=\u001b[39m[top_k], n_workers\u001b[38;5;241m=\u001b[39mn_workers, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     64\u001b[0m     problem_file\u001b[38;5;241m=\u001b[39mreference_file)\n\u001b[1;32m     65\u001b[0m model_execution_results \u001b[38;5;241m=\u001b[39m model_outputs_jsonl \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_results.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "codellama_trial.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174d8c98-2cde-4700-b1f9-dfaa64379f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(clw)\n",
    "clt = clw.CodellamaEval(model_name = None, method_dict_location = 'output_multi-humaneval_CodeLlama-7b-hf.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecbe71f0-9b28-4196-b54e-4a5c4d680b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humaneval\n",
      "aaa\n",
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [00:00, 18255.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 161/161 [01:17<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to outputmulti-humaneval.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 42755.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pass rate: 22.36, test fail rate: 42.86, compilation error rate: 34.16, out of time rate: 0.62'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codellama_wrapper as clw\n",
    "clt = clw.CodellamaEval(model_name = None, method_dict_location = 'output_multi-humaneval_CodeLlama-7b-hf.json')\n",
    "clt.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38ba1926-45cd-468c-a6d6-0c829b264828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [00:00, 14615.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 161/161 [01:15<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to outputmulti-humaneval.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 24300.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected behavior from mxeval\n",
      "Command '['java', '-jar', '/home/evtikhiev/eval/mxeval/mxeval/kotlin_exec_eval/aAJlGluhEK.jar']' timed out after 15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pass rate: 22.36, test fail rate: 42.86, compilation error rate: 34.16'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codellama_trial.codellama_evaluate(problem_name = 'humaneval', model_outputs_jsonl = 'outputmulti-humaneval.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adb8eead-26df-4b66-90a2-ddbac499e284",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m codet5 \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper\u001b[49m\u001b[38;5;241m.\u001b[39mKotlinLLMEval(model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalesforce/codet5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wrapper' is not defined"
     ]
    }
   ],
   "source": [
    "codet5 = wrapper.KotlinLLMEval(model_name = \"Salesforce/codet5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e95278-e092-4712-a099-7551766365b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codellama_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aac7dd0-65c5-4f3d-929d-669549157bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CodellamaEval',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'wrapper']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(codellama_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf7b724-80e6-46a2-902c-d8736b99fa62",
   "metadata": {},
   "source": [
    "### Starcoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad1db94e-181c-46a5-b9d2-ec9ff1d27a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325518b9a956437c964ec2663794bfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "/home/evtikhiev/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/evtikhiev/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1535: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_hello_world():\n",
      "    print(\"Hello World\")\n",
      "\n",
      "print_hello_world\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/starcoderbase-7b\"\n",
    "device = \"cuda:2\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, load_in_8bit=True)\n",
    "\n",
    "inputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c658668-53ca-488d-9e48-f455a41842c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfad6c56-9f04-4298-b33e-3088e60970f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949c70c5ea4d45a989922279a3c1d5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/717 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b914b5f2aedd4a96a6cade46996b4a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4491e3af0e4b96bb257757cd717be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208de3ed477646d4a2b87e2a17eecf5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f187116cb2e84b93ab4c33447e5970d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/532 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c83fae634b847a4a2210d17c09aaa39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5fa25b45df49b2a3bb51b048181c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ration_gpt_refact.py:   0%|          | 0.00/1.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/smallcloudai/Refact-1_6B-fim:\n",
      "- configuration_gpt_refact.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f9690de7454cc58c87b59ba26bc060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modeling_gpt_refact.py:   0%|          | 0.00/24.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/smallcloudai/Refact-1_6B-fim:\n",
      "- modeling_gpt_refact.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8dc6055fd6140a79c2ccff2676e391f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8626144d0842b3a3e9657199648433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "<fim_prefix>def print_hello_world():\n",
      "    \"\"\"<fim_suffix>\n",
      "    print(\"Hello world!\")<fim_middle>\n",
      "    Print \"Hello world!\"\n",
      "    \"\"\"<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"smallcloudai/Refact-1_6B-fim\"\n",
    "device = \"cuda:2\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)\n",
    "\n",
    "prompt = '<fim_prefix>def print_hello_world():\\n    \"\"\"<fim_suffix>\\n    print(\"Hello world!\")<fim_middle>'\n",
    "\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_length=200, temperature=0.2)\n",
    "print(\"-\"*80)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c72a95b7-130a-4c90-bcbe-6328726fc7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "/**\n",
      " * You are an expert Kotlin programmer, and here is your task.\n",
      " * Insert a number 'delimeter' between every two consecutive elements of input list `numbers'\n",
      " * >>> intersperse([], 4)\n",
      " * []\n",
      " * >>> intersperse([1, 2, 3], 4)\n",
      " * [1, 4, 2, 4, 3]\n",
      " *\n",
      " */\n",
      "fun intersperse(numbers : List<Any>, delimeter : Int) : List<Any> {\n",
      "    val result = mutableListOf<Any>()\n",
      "    for (number in numbers) {\n",
      "        result.add(number)\n",
      "        if (result.size == 2) {\n",
      "            result.add(delimeter)\n",
      "        }\n",
      "    }\n",
      "    return result\n",
      "}\n",
      "\n",
      "/**\n",
      " * Test cases\n",
      " */\n",
      "fun main() {\n",
      "    // Test case 1\n",
      "    val numbers1 = listOf(1, 2, 3)\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "prompt = \"/**\\n * You are an expert Kotlin programmer, and here is your task.\\n * Insert a number 'delimeter' between every two consecutive elements of input list `numbers'\\n * >>> intersperse([], 4)\\n * []\\n * >>> intersperse([1, 2, 3], 4)\\n * [1, 4, 2, 4, 3]\\n *\\n */\\nfun intersperse(numbers : List<Any>, delimeter : Int) : List<Any> {\\n\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_length=200, temperature=0.2)\n",
    "print(\"-\"*80)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7c3c42-5f10-4895-a533-e68a60d8e5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
