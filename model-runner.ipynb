{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30ff65f1-59dd-4c5e-8598-08b3b5d5aace",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(\u001b[43mwrapper\u001b[49m)\n\u001b[1;32m      3\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(clw)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wrapper' is not defined"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(clw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f241a8baeea1e8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b0384e899449adab4f695636e7979f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading multi-humaneval | language = kotlin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "1it [00:12, 13.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "2it [00:25, 12.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "3it [00:38, 12.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "4it [00:50, 12.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "5it [01:03, 12.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "6it [01:16, 12.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "7it [01:28, 12.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "8it [01:42, 12.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "9it [01:54, 12.79s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "10it [02:07, 12.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "11it [02:19, 12.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "12it [02:33, 12.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "13it [02:46, 12.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "14it [03:00, 13.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "15it [03:14, 13.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "16it [03:28, 13.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "17it [03:42, 13.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "18it [03:52, 12.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "19it [04:06, 12.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "20it [04:18, 12.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "21it [04:29, 12.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "22it [04:41, 12.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "23it [04:55, 12.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "24it [05:09, 13.22s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "25it [05:24, 13.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "26it [05:36, 13.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "27it [05:50, 13.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "28it [06:04, 13.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "29it [06:19, 13.89s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "30it [06:32, 13.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "31it [06:44, 13.21s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "32it [06:56, 12.95s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "33it [07:07, 12.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "34it [07:21, 12.77s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "35it [07:35, 12.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "36it [07:48, 13.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "37it [08:00, 12.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "38it [08:12, 12.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "39it [08:22, 11.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "40it [08:33, 11.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "41it [08:45, 11.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "42it [08:55, 11.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "43it [09:07, 11.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "44it [09:22, 12.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "45it [09:32, 11.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "46it [09:46, 12.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "47it [09:59, 12.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "48it [10:11, 12.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "49it [10:23, 12.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "50it [10:36, 12.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "51it [10:50, 13.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "52it [11:01, 12.29s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "53it [11:15, 12.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "54it [11:28, 12.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "55it [11:41, 12.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "56it [11:53, 12.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "57it [12:06, 12.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "58it [12:19, 12.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "59it [12:32, 12.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "60it [12:44, 12.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "61it [12:54, 12.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "62it [13:06, 11.92s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "63it [13:19, 12.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "64it [13:31, 12.15s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "65it [13:39, 10.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "66it [13:42,  8.59s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "67it [13:53,  9.12s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "68it [14:04,  9.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "69it [14:16, 10.46s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "70it [14:23,  9.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "71it [14:34,  9.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "72it [14:42,  9.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "73it [14:56, 10.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "74it [15:07, 10.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "75it [15:19, 11.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "76it [15:23,  9.11s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "77it [15:34,  9.66s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "78it [15:46, 10.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "79it [15:51,  8.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "80it [16:05, 10.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "81it [16:19, 11.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "82it [16:30, 11.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "83it [16:44, 12.13s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "84it [16:55, 11.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "85it [17:02, 10.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "86it [17:10,  9.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "87it [17:22, 10.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "88it [17:34, 10.80s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "89it [17:46, 11.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "90it [17:57, 11.03s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "91it [18:09, 11.27s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "92it [18:14,  9.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "93it [18:23,  9.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "94it [18:33,  9.57s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "95it [18:45, 10.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "96it [18:59, 11.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "97it [19:09, 10.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "98it [19:20, 11.06s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "99it [19:32, 11.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100it [19:45, 11.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "101it [19:55, 11.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "102it [20:07, 11.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "103it [20:12,  9.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "104it [20:24, 10.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "105it [20:32,  9.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "106it [20:43, 10.10s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "107it [20:48,  8.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "108it [20:57,  8.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "109it [21:07,  9.18s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "110it [21:18,  9.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "111it [21:27,  9.52s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "112it [21:40, 10.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "113it [21:44,  8.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "114it [21:54,  8.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "115it [22:03,  9.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "116it [22:13,  9.43s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "117it [22:24,  9.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "118it [22:31,  9.07s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "119it [22:44, 10.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "120it [22:55, 10.48s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "121it [23:03,  9.72s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "122it [23:09,  8.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "123it [23:20,  9.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "124it [23:27,  8.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "125it [23:34,  8.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "126it [23:46,  9.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "127it [23:47,  6.83s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "128it [23:54,  7.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "129it [24:08,  9.09s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "130it [24:19,  9.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "131it [24:30,  9.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "132it [24:40, 10.05s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "133it [24:53, 10.81s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "134it [25:04, 10.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "135it [25:15, 10.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "136it [25:28, 11.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "137it [25:41, 11.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "138it [25:53, 11.99s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "139it [26:01, 10.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "140it [26:11, 10.70s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "141it [26:22, 10.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "142it [26:32, 10.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "143it [26:44, 10.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "144it [26:56, 11.19s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "145it [27:06, 10.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "146it [27:13,  9.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "147it [27:23,  9.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "148it [27:35, 10.60s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "149it [27:46, 10.69s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "150it [27:54,  9.76s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "151it [28:00,  8.65s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "152it [28:11,  9.47s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "153it [28:25, 10.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "154it [28:37, 11.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "155it [28:49, 11.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "156it [29:01, 11.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "157it [29:05,  9.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "158it [29:13,  8.88s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "159it [29:25,  9.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "160it [29:38, 10.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "161it [29:50, 11.12s/it]\n"
     ]
    }
   ],
   "source": [
    "import codellama_wrapper as clw\n",
    "codellama_trial = clw.CodellamaEval(model_name = \"codellama/CodeLlama-7b-hf\", gpu_id=1)\n",
    "codellama_trial.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f00e98d8-4eda-4303-8495-878db904c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = codellama_trial.model_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb3a09a0-4c8f-4336-8c62-da739f54e168",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcodellama_trial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eval/mxeval/codellama_wrapper.py:66\u001b[0m, in \u001b[0;36mCodellamaEval.evaluate\u001b[0;34m(problem_name, model_outputs_jsonl, top_k, n_workers, timeout)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(problem_name: \u001b[38;5;28mstr\u001b[39m  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumaneval\u001b[39m\u001b[38;5;124m'\u001b[39m, model_outputs_jsonl: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputmulti-humaneval.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15.0\u001b[39m):\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_outputs_jsonl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eval/mxeval/wrapper.py:61\u001b[0m, in \u001b[0;36mKotlinLLMEval.evaluate\u001b[0;34m(self, problem_name, model_outputs_jsonl, top_k, n_workers, timeout)\u001b[0m\n\u001b[1;32m     59\u001b[0m     reference_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/mbxp/mbkp_release_v1.2.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m     62\u001b[0m mxeval\u001b[38;5;241m.\u001b[39mevaluation\u001b[38;5;241m.\u001b[39mevaluate_functional_correctness(\n\u001b[1;32m     63\u001b[0m     sample_file\u001b[38;5;241m=\u001b[39mmodel_outputs_jsonl, k\u001b[38;5;241m=\u001b[39m[top_k], n_workers\u001b[38;5;241m=\u001b[39mn_workers, timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m     64\u001b[0m     problem_file\u001b[38;5;241m=\u001b[39mreference_file)\n\u001b[1;32m     65\u001b[0m model_execution_results \u001b[38;5;241m=\u001b[39m model_outputs_jsonl \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_results.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "codellama_trial.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174d8c98-2cde-4700-b1f9-dfaa64379f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(clw)\n",
    "clt = clw.CodellamaEval(model_name = None, method_dict_location = 'output_multi-humaneval_CodeLlama-7b-hf.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecbe71f0-9b28-4196-b54e-4a5c4d680b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humaneval\n",
      "aaa\n",
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [00:00, 18255.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 161/161 [01:17<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to outputmulti-humaneval.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 42755.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pass rate: 22.36, test fail rate: 42.86, compilation error rate: 34.16, out of time rate: 0.62'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codellama_wrapper as clw\n",
    "clt = clw.CodellamaEval(model_name = None, method_dict_location = 'output_multi-humaneval_CodeLlama-7b-hf.json')\n",
    "clt.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38ba1926-45cd-468c-a6d6-0c829b264828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [00:00, 14615.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test suites...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 161/161 [01:15<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing results to outputmulti-humaneval.jsonl_results.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:00<00:00, 24300.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected behavior from mxeval\n",
      "Command '['java', '-jar', '/home/evtikhiev/eval/mxeval/mxeval/kotlin_exec_eval/aAJlGluhEK.jar']' timed out after 15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pass rate: 22.36, test fail rate: 42.86, compilation error rate: 34.16'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codellama_trial.codellama_evaluate(problem_name = 'humaneval', model_outputs_jsonl = 'outputmulti-humaneval.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adb8eead-26df-4b66-90a2-ddbac499e284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a27a672afa44f3cae1ee0b3b45c3d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c875c58118a74cc283269ed10da87aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/511k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3e1654fb4a4b48aa149f0fe59af57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/294k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c3107c59c649dea31810e4d8c02f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93de995604dd4800a3ddac6aa6738bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/11.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebed26af7414e279b4752df9ba9676d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model Salesforce/codet5-large with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/evtikhiev/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/evtikhiev/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, ElectraConfig, ErnieConfig, FalconConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, LlamaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MptConfig, MusicgenConfig, MvpConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, TransfoXLConfig, TrOCRConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m codet5 \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKotlinLLMEval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSalesforce/codet5-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eval/mxeval/wrapper.py:26\u001b[0m, in \u001b[0;36mKotlinLLMEval.__init__\u001b[0;34m(self, model_name, gpu_id, method_dict_location)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/__init__.py:824\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    823\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 824\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    835\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:282\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    281\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    283\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         )\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model Salesforce/codet5-large with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/evtikhiev/.local/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/evtikhiev/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, ElectraConfig, ErnieConfig, FalconConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, LlamaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MptConfig, MusicgenConfig, MvpConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, TransfoXLConfig, TrOCRConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig.\n\n\n"
     ]
    }
   ],
   "source": [
    "codet5 = wrapper.KotlinLLMEval(model_name = \"Salesforce/codet5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e95278-e092-4712-a099-7551766365b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codellama_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aac7dd0-65c5-4f3d-929d-669549157bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CodellamaEval',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'wrapper']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(codellama_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c47c0f-1033-4c0e-9a87-a279bd27fed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
